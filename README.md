# OCTI Realtime Backend

Backend WebSocket pour l'agent IA vocal OCTI utilisant l'API OpenAI Realtime (GPT-4o Realtime Preview).

## ðŸŽ¯ Objectif

Fournir un backend simple, fiable et rÃ©utilisable qui fait le proxy entre le frontend et l'API OpenAI Realtime pour permettre une communication speech-to-speech en temps rÃ©el avec une latence minimale.

## ðŸš€ DÃ©marrage rapide

### PrÃ©requis

- Node.js â‰¥ 20
- npm ou yarn
- ClÃ© API OpenAI avec accÃ¨s Ã  l'API Realtime

### Installation

```bash
# Installer les dÃ©pendances
npm install

# Copier le fichier d'environnement
cp .env.example .env

# Ã‰diter .env et remplir vos variables
# Notamment OPENAI_API_KEY et OCTI_SYSTEM_PROMPT
```

### Configuration (.env)

```env
PORT=8080
NODE_ENV=development
OPENAI_API_KEY=sk-xxx
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview
OCTI_SYSTEM_PROMPT="Tu es OCTI, l'assistant vocal intelligent..."
OCTI_DEFAULT_VOICE=alloy
OCTI_INPUT_AUDIO_FORMAT=pcm16
OCTI_OUTPUT_AUDIO_FORMAT=pcm16
```

### Lancer en dÃ©veloppement

```bash
npm run dev
```

### Build et production

```bash
# Compiler TypeScript
npm run build

# Lancer le serveur
npm start
```

Le serveur dÃ©marre sur `http://localhost:8080` (ou le port configurÃ©).

## ðŸ“¡ Protocole WebSocket

### Endpoint

```
wss://<BACKEND_DOMAIN>/ws/realtime
```

### Messages Frontend â†’ Backend

#### 1. DÃ©marrer la conversation

```json
{ "type": "start_conversation" }
```

#### 2. Envoyer un chunk audio (binaire)

EnvoyÃ© en `ArrayBuffer` (PCM16), pas de JSON :

```javascript
ws.send(pcm16Buffer);
```

#### 3. Fin de la parole utilisateur

```json
{ "type": "user_audio_end" }
```

#### 4. Reset session

```json
{ "type": "reset_session" }
```

### Messages Backend â†’ Frontend

#### 1. Backend prÃªt

```json
{ "type": "ready" }
```

EnvoyÃ© automatiquement lorsque la session Realtime est initialisÃ©e.

#### 2. Chunk audio du modÃ¨le (binaire)

Audio PCM16 Ã  jouer directement. ReÃ§u en `ArrayBuffer`.

#### 3. Fin de la rÃ©ponse vocale

```json
{ "type": "bot_audio_end" }
```

#### 4. Transcription texte (optionnel, pour affichage)

```json
{ "type": "transcript_delta", "text": "..." }
```

#### 5. Erreur

```json
{ "type": "error", "message": "..." }
```

## ðŸ—ï¸ Architecture

```
src/
  server.ts                 # Point d'entrÃ©e du serveur
  app/
    index.ts                # Configuration Express
    httpRoutes/
      healthRoute.ts        # Route GET /health
    wsHandlers/
      realtimeHandler.ts    # Handler WebSocket principal
  core/
    realtime/
      OpenAIRealtimeClient.ts  # Client WebSocket OpenAI
      types.ts                 # Types pour l'API Realtime
    agents/
      AgentConfig.ts          # Configuration gÃ©nÃ©rique d'agent
      octiAgent.ts            # Configuration spÃ©cifique OCTI
    sessions/
      SessionManager.ts       # Gestionnaire de sessions
  config/
    env.ts                    # Configuration environnement
    logger.ts                 # Logger Pino
  utils/
    wsMessages.ts             # Types et helpers messages WS
    errors.ts                 # Erreurs personnalisÃ©es
```

## ðŸ”§ Routes HTTP

### GET /health

VÃ©rifie que le serveur est opÃ©rationnel.

**RÃ©ponse :**
```json
{
  "status": "ok",
  "timestamp": "2024-01-01T12:00:00.000Z",
  "service": "octi-realtime-backend"
}
```

## ðŸ“ Exemple d'utilisation (Frontend)

```javascript
const ws = new WebSocket('wss://your-backend.com/ws/realtime');

ws.onopen = () => {
  console.log('Connexion Ã©tablie');
};

ws.onmessage = (event) => {
  // Message JSON
  if (typeof event.data === 'string') {
    const message = JSON.parse(event.data);
    
    switch (message.type) {
      case 'ready':
        console.log('Backend prÃªt');
        break;
      case 'bot_audio_end':
        console.log('RÃ©ponse audio terminÃ©e');
        break;
      case 'transcript_delta':
        console.log('Transcription:', message.text);
        break;
      case 'error':
        console.error('Erreur:', message.message);
        break;
    }
  } 
  // Audio binaire (PCM16)
  else {
    const audioBuffer = event.data;
    // Jouer l'audio
    playAudio(audioBuffer);
  }
};

// DÃ©marrer la conversation
ws.send(JSON.stringify({ type: 'start_conversation' }));

// Envoyer un chunk audio
ws.send(audioChunk);

// Signaler la fin de l'audio utilisateur
ws.send(JSON.stringify({ type: 'user_audio_end' }));
```

## ðŸš¢ DÃ©ploiement sur Render

1. CrÃ©er un nouveau **Web Service** sur Render
2. Connecter votre repository GitHub
3. Configurer :
   - **Build Command** : `npm install && npm run build`
   - **Start Command** : `npm start`
   - **Environment Variables** : Ajouter toutes les variables de `.env.example`
4. DÃ©ployer

Le service sera accessible sur `https://your-service.onrender.com`

## ðŸ§ª Tests

Pour tester la latence et le fonctionnement :

1. VÃ©rifier que le serveur rÃ©pond : `curl http://localhost:8080/health`
2. Tester la connexion WebSocket avec un client WebSocket
3. Envoyer des chunks audio PCM16 et vÃ©rifier la rÃ©ception de l'audio en retour

## ðŸ“¦ DÃ©pendances

- **express** : Serveur HTTP
- **ws** : WebSocket
- **dotenv** : Variables d'environnement
- **pino** : Logger performant
- **typescript** : Compilation TypeScript

## ðŸ”’ SÃ©curitÃ©

- Ne jamais commiter le fichier `.env`
- Utiliser des variables d'environnement pour les secrets
- Valider tous les messages WebSocket entrants
- GÃ©rer proprement les erreurs et fermer les connexions

## ðŸ“„ Licence

MIT

